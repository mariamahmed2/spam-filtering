
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nJDbzFJXs5YsvGvw4WOiHsapkWD-pcIb

# **Spam Filtering Using The Multinomial Naive Bayes Algorithm**
"""

import pandas as pd
from IPython.display import Image
import matplotlib.pyplot as plt

"""## 1- Data Handling

- Reading the dataset and nameing the columns
"""

df = pd.read_csv('emails', sep='\t',header=None, names=['Label', 'SMS'])
print(df.shape)
df.head()

"""- Cleaning the data"""

df.describe()

df.duplicated().sum()

dataset = df.copy()
dataset= dataset.drop_duplicates()
dataset.dropna(subset=['Label'],inplace=True)

dataset.shape

"""- Getting the probability of Ham and Spam sms"""

dataset['Label'].value_counts(normalize=True)

"""- Splitting the dataset into train and test sets with the same ratio"""

# Calculate index for split
training_test_index = round(len(dataset) * 0.8)

# Split into training and test sets
training_set = dataset[:training_test_index].reset_index(drop=True)
test_set = dataset[training_test_index:].reset_index(drop=True)

print(training_set.shape)
print(test_set.shape)

training_set['Label'].value_counts(normalize=True)

test_set['Label'].value_counts(normalize=True)

# Before cleaning
training_set.head(3)

"""- Removing all special chars 
- Converting to lowercase
"""

# After cleaning
training_set['SMS'] = training_set['SMS'].str.replace(
   '\W', ' ') # Removes punctuation
training_set['SMS'] = training_set['SMS'].str.lower()
training_set.head(3)

"""- Creating the Vocabulary

"""

training_set['SMS'] = training_set['SMS'].str.split()

vocabulary = []
for sms in training_set['SMS']:
   for word in sms:
      vocabulary.append(word)

vocabulary = list(set(vocabulary))

len(vocabulary)

vocabulary[41:50]

"""- creating the dictionary and concatenating it with the dataset"""

word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}

for index, sms in enumerate(training_set['SMS']):
   for word in sms:
      word_counts_per_sms[word][index] += 1

word_counts = pd.DataFrame(word_counts_per_sms)
word_counts.head()

training_set_clean = pd.concat([training_set, word_counts], axis=1)
training_set_clean.head()

"""## 2- Building the algorithm"""

# Isolating spam and ham messages first
spam_messages = training_set_clean[training_set_clean['Label'] == 'spam']
ham_messages = training_set_clean[training_set_clean['Label'] == 'ham']

"""- Calculating the constants"""

Image('1.png')

# P(Spam) and P(Ham)
p_spam = len(spam_messages) / len(training_set_clean)
p_ham = len(ham_messages) / len(training_set_clean)
print(p_spam)
print(p_ham)

# N_Spam
n_words_per_spam_message = spam_messages['SMS'].apply(len)
n_spam = n_words_per_spam_message.sum()

# N_Ham
n_words_per_ham_message = ham_messages['SMS'].apply(len)
n_ham = n_words_per_ham_message.sum()

plt.figure(figsize=(8, 8))

n_words_per_ham_message.plot(bins=35, kind='hist', color='blue', label='Ham messages', alpha=0.6)
n_words_per_spam_message.plot(kind='hist', color='red', label='Spam messages', alpha=0.6)

plt.legend()
plt.xlabel("Message Length")

# N_Vocabulary
n_vocabulary = len(vocabulary)

# Laplace smoothing
alpha = 1

"""- Calculating the parameters"""

# Initiate parameters
parameters_spam = {unique_word:0 for unique_word in vocabulary}
parameters_ham = {unique_word:0 for unique_word in vocabulary}

Image('2.png')

# Calculate parameters
for word in vocabulary:
   n_word_given_spam = spam_messages[word].sum() # spam_messages already defined
   p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)
   parameters_spam[word] = p_word_given_spam

   n_word_given_ham = ham_messages[word].sum() # ham_messages already defined
   p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)
   parameters_ham[word] = p_word_given_ham

"""- Classifying the message"""

import re

def classify(message):
   '''
   message: a string
   '''

   message = re.sub('\W', ' ', message)
   message = message.lower().split()

   p_spam_given_message = p_spam
   p_ham_given_message = p_ham

   for word in message:
      if word in parameters_spam:
         p_spam_given_message *= parameters_spam[word]

      if word in parameters_ham: 
         p_ham_given_message *= parameters_ham[word]

   print('P(Spam|message):', p_spam_given_message)
   print('P(Ham|message):', p_ham_given_message)

   if p_ham_given_message > p_spam_given_message:
      print('Ham')
   elif p_ham_given_message < p_spam_given_message:
      print('Spam')
   else:
      print('Equal proabilities, have a human classify this!')

classify('Kill code secret Fail')

classify("What a good day ")

"""## 3- Measuring the Accuracy"""

def classify_test_set(message):
   '''
   message: a string
   '''

   message = re.sub('\W', ' ', message)
   message = message.lower().split()

   p_spam_given_message = p_spam
   p_ham_given_message = p_ham

   for word in message:
      if word in parameters_spam:
         p_spam_given_message *= parameters_spam[word]

      if word in parameters_ham:
         p_ham_given_message *= parameters_ham[word]

   if p_ham_given_message > p_spam_given_message:
      return 'ham'
   elif p_spam_given_message > p_ham_given_message:
      return 'spam'
   else:
      return 'needs human classification'

test_set['predicted'] = test_set['SMS'].apply(classify_test_set)
test_set.head()

Image('3.png')

correct = 0
total = test_set.shape[0]

for row in test_set.iterrows():
   row = row[1]
   if row['Label'] == row['predicted']:
      correct += 1

print('Correct:', correct)
print('Incorrect:', total - correct)
print('Accuracy:', correct/total)
